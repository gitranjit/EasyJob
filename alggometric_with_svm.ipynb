{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3bfde-d89f-4b5b-aa5e-65fc46accd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algglomerative clustering with svm classifier\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  # Import Support Vector Classifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = ''\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to extract skills from text using a skills dataset\n",
    "def extract_skills(text, skills_list):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    skills = [token for token in tokens if token.lower() in skills_list and token.lower() not in stop_words]\n",
    "    return skills\n",
    "\n",
    "# Load skills data from CSV\n",
    "skills_data = pd.read_csv('skills.csv')\n",
    "skills_list = set(skills_data['Skills'].str.lower().str.split().explode().tolist())\n",
    "\n",
    "# Load job descriptions from Excel\n",
    "job_data = pd.read_csv('indeed_data.csv')  # Update with your file name and path\n",
    "job_descriptions = job_data['description'].tolist()\n",
    "\n",
    "# Preprocess job descriptions\n",
    "preprocessed_job_descriptions = [preprocess_text(desc) for desc in job_descriptions]\n",
    "\n",
    "# Extract skills from job descriptions\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_job = vectorizer.fit_transform(preprocessed_job_descriptions)\n",
    "\n",
    "# Cluster job descriptions using Agglomerative Clustering\n",
    "num_clusters = 5  # You can adjust the number of clusters as needed\n",
    "agglomerative = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "job_clusters = agglomerative.fit_predict(X_job.toarray())\n",
    "\n",
    "# Split labeled data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_job_descriptions, job_clusters, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model to predict clusters\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "classifier = SVC()  # Use SVM classifier\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "predicted_clusters = classifier.predict(X_test_vec)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predicted_clusters)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, predicted_clusters, average='weighted')\n",
    "recall = recall_score(y_test, predicted_clusters, average='weighted')\n",
    "f1 = f1_score(y_test, predicted_clusters, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Extract skills from resume\n",
    "resume_file = 'resume_yash.pdf'  # Replace with your PDF file path\n",
    "resume_text = extract_text_from_pdf(resume_file)\n",
    "preprocessed_resume = preprocess_text(resume_text)\n",
    "resume_skills = extract_skills(preprocessed_resume, skills_list)\n",
    "\n",
    "# Assign resume to the cluster with the most similar job descriptions\n",
    "resume_vec = vectorizer.transform([preprocessed_resume])\n",
    "predicted_cluster = classifier.predict(resume_vec)[0]\n",
    "\n",
    "# Get job titles and URLs for the predicted cluster\n",
    "predicted_jobs = job_data[job_clusters == predicted_cluster]\n",
    "\n",
    "print(\"Predicted Cluster:\", predicted_cluster)\n",
    "print(\"Resume Skills:\", resume_skills)\n",
    "print(\"Predicted Jobs:\")\n",
    "job_similarities = []\n",
    "\n",
    "for idx, job in predicted_jobs.iterrows():\n",
    "    job_description_vec = vectorizer.transform([preprocess_text(job['description'])])\n",
    "    similarity = cosine_similarity(resume_vec, job_description_vec)[0][0]\n",
    "    job_similarities.append((job['title'], job['link'], similarity))\n",
    "\n",
    "# Sort jobs by cosine similarity and print top 5\n",
    "top_5_jobs = sorted(job_similarities, key=lambda x: x[2], reverse=True)[:5]\n",
    "print(\"Top 5 Jobs:\")\n",
    "for title, url, similarity in top_5_jobs:\n",
    "    print(\"Job Title:\", title)\n",
    "    print(\"Job URL:\", url)\n",
    "    print(\"Cosine Similarity:\", similarity)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
